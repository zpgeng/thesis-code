{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE Latent Confounder Finder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okccl8zJrtWG"
      },
      "source": [
        "## Latent Confounder Finder\n",
        "\n",
        "This is the code for finding the estimated latent confounders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ufy7VrQMR2p"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "from scipy.stats import norm\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWyFp02KsxlJ"
      },
      "source": [
        "Here the large dataset and small dataset are generated using \n",
        "\n",
        "```\n",
        "Export_dataset.R\n",
        "```\n",
        "\n",
        "You should first generate from R and import them here for the cross-language reproducibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5BsulpSz4Fi"
      },
      "source": [
        "ldata = pd.read_csv(\"/content/drive/MyDrive/Thesis & Project/largedata.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MylgGqXv5N6C"
      },
      "source": [
        "ldata = ldata.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8hbcpx053ax"
      },
      "source": [
        "sdata = pd.read_csv(\"/content/drive/MyDrive/Thesis & Project/smalldata.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K4T9Wir5-J6"
      },
      "source": [
        "sdata = sdata.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vNi2Lnh1FeR",
        "outputId": "d9ebff24-7c29-4006-99d5-9cfa3b77ddc7"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"GPU Enabled:\",torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Enabled: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTjbS4x1QL25"
      },
      "source": [
        "def setup_data_loaders(amount, batch_size=200, use_cuda=False):\n",
        "  if amount == \"L\":\n",
        "    datasets = pd.read_csv(\"/content/drive/MyDrive/Thesis & Project/largedata.csv\")\n",
        "  else:\n",
        "    datasets = pd.read_csv(\"/content/drive/MyDrive/Thesis & Project/smalldata.csv\")\n",
        "  scaler = preprocessing.MinMaxScaler()\n",
        "  names = datasets.columns\n",
        "  d = scaler.fit_transform(datasets)\n",
        "  scaled_df = pd.DataFrame(d, columns=names)\n",
        "  scaled_data = scaled_df.to_numpy()\n",
        "  train_set = scaled_data[range(0, int(len(datasets) / 5 * 4)), ].astype(np.float32)\n",
        "  test_set = scaled_data[range(int(len(datasets) / 5 * 4), len(datasets)), ].astype(np.float32)\n",
        "  data_loader = DataLoader(dataset=scaled_data.astype(np.float32), batch_size=len(datasets), shuffle=True)\n",
        "  train_loader = DataLoader(dataset=train_set,\n",
        "                            batch_size=batch_size, shuffle=True)\n",
        "  test_loader = DataLoader(dataset=test_set,\n",
        "                           batch_size=batch_size, shuffle=False)\n",
        "  return data_loader, train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8ZauaxdudMn"
      },
      "source": [
        "## Defining VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlKtRSZR2cPU"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(nn.Linear(20, 256),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.Linear(256, 128),\n",
        "                                     nn.BatchNorm1d(128),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.Linear(128, 64),\n",
        "                                     nn.BatchNorm1d(64),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.Linear(64, 8),\n",
        "                                     )\n",
        "        \n",
        "        self.mu     = nn.Linear(8, latent_dim)\n",
        "        self.logvar = nn.Linear(8, latent_dim)\n",
        "        \n",
        "        self.latent_mapping = nn.Linear(latent_dim, 8)\n",
        "        \n",
        "        self.decoder = nn.Sequential(nn.Linear(8, 16),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.BatchNorm1d(16),\n",
        "                                     nn.Linear(16, 64),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.BatchNorm1d(64),\n",
        "                                     nn.Linear(64, 128),\n",
        "                                     nn.Softplus(),\n",
        "                                     nn.BatchNorm1d(128),\n",
        "                                     nn.Linear(128, 20))        \n",
        "        \n",
        "    def encode(self, x):\n",
        "        #x = x.view(x.size(0), -1)\n",
        "        encoder = self.encoder(x)\n",
        "        mu, logvar = self.mu(encoder), self.logvar(encoder)\n",
        "        return mu, logvar\n",
        "        \n",
        "    def sample_z(self, mu, logvar):\n",
        "        eps = torch.rand_like(mu)\n",
        "        return mu + eps * torch.exp(0.5 * logvar)\n",
        "    \n",
        "    def decode(self, z, x):\n",
        "        latent_z = self.latent_mapping(z)\n",
        "        out = self.decoder(latent_z)\n",
        "        reshaped_out = torch.sigmoid(out).reshape((-1, 20))\n",
        "        return reshaped_out\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.sample_z(mu, logvar)\n",
        "        output = self.decode(z, x)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qmN1TDe2vMR"
      },
      "source": [
        "def elbo_loss(x_generated, x_true, mu, logvar):\n",
        "    recon_loss = nn.functional.mse_loss(x_generated, x_true, reduction='none')\n",
        "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), 1).mean()\n",
        "    loss = torch.mean(kld_loss + recon_loss)\n",
        "    \n",
        "    return loss, torch.mean(recon_loss), torch.mean(kld_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIHkZODE4ser"
      },
      "source": [
        "# Define the functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGu6PSPr4hSM"
      },
      "source": [
        "def training_function(latent_dimension, train, test):\n",
        "  vae_net = VAE(latent_dim = latent_dimension)\n",
        "  opt = torch.optim.Adam(vae_net.parameters())\n",
        "  BATCH_SIZE = 200\n",
        "  max_epochs = 30\n",
        "\n",
        "  vae_net = vae_net.to(device)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "      \n",
        "      train_loss = 0.0\n",
        "      train_loss_rec = 0.0\n",
        "      train_loss_kdl = 0.0\n",
        "      \n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "          inputs = data\n",
        "\n",
        "          inputs = inputs.to(device)\n",
        "          \n",
        "          # training steps for normal model\n",
        "          opt.zero_grad()\n",
        "          \n",
        "          mu, logvar = vae_net.encode(inputs)\n",
        "          z = vae_net.sample_z(mu, logvar)\n",
        "          outputs = vae_net.decode(z, inputs)\n",
        "\n",
        "          loss, recon_loss, kld_loss = elbo_loss(outputs, inputs, mu, logvar)\n",
        "          loss.backward()\n",
        "          opt.step()   \n",
        "        \n",
        "          # print statistics\n",
        "          train_loss += loss.item()\n",
        "          train_loss_rec += recon_loss.item()\n",
        "          train_loss_kdl += kld_loss.item()\n",
        "\n",
        "    \n",
        "      test_loss = 0.0\n",
        "      test_loss_rec = 0.0\n",
        "      test_loss_kdl = 0.0\n",
        "\n",
        "      for i, data in enumerate(test_loader, 0):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        mu, logvar = vae_net.encode(inputs)\n",
        "        z = vae_net.sample_z(mu, logvar)\n",
        "        outputs = vae_net.decode(z, inputs)\n",
        "        \n",
        "        loss, recon_loss, kld_loss = elbo_loss(outputs, inputs, mu, logvar)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        test_loss_rec += recon_loss.item()\n",
        "        test_loss_kdl += kld_loss.item()\n",
        "\n",
        "\n",
        "      print(f'Epoch {epoch+1} \\t\\t Training Loss: {\\\n",
        "                                              train_loss / len(train_loader)} \\t\\t Validation Loss: {\\\n",
        "                                                                                                      test_loss / len(test_loader)}')\n",
        "  return vae_net\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wNYly97uVOo"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU8vE6CK6Fp9"
      },
      "source": [
        "def save_z(amount, num_hidden):\n",
        "  data_loader, train_loader, test_loader = setup_data_loaders(amount)\n",
        "  vae_net = training_function(num_hidden, train_loader, test_loader)\n",
        "  for _, data in enumerate(data_loader, 0):\n",
        "    inputs = data.to(device)\n",
        "    mu, logvar = vae_net.encode(inputs)\n",
        "    z = vae_net.sample_z(mu, logvar)\n",
        "    z_np = z.cpu().detach().numpy() #convert to Numpy array\n",
        "    df = pd.DataFrame(z_np) #convert to a dataframe\n",
        "    df.to_csv(\"/content/vae/\" + str(amount) + str(num_hidden) + \"file.csv\", index=False) #save to file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZtCO6Ca7mpR"
      },
      "source": [
        "!mkdir vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNGRuYxNtu1R"
      },
      "source": [
        "Started to train and save the corresponding latent confounder estimations. Then you can import them in R. With the same random seed in R, it can be guaranteed to experiment on the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jrKiG_XMzEO"
      },
      "source": [
        "for _ in range(1, 21):\n",
        "  save_z(\"L\", _)\n",
        "\n",
        "for _ in range(1, 21):\n",
        "  save_z(\"S\", _)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eGZLtf--Z0N"
      },
      "source": [
        "!zip -r latentvars.zip /content/vae/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzYEqUk9uRdq"
      },
      "source": [
        "## Model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNffd9dO6wpc"
      },
      "source": [
        "from torchvision import models\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jODDOOpz7IEu"
      },
      "source": [
        "summary(vae_net, input_size=(20, ))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}